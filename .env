# No cloud API keys needed for local Ollama
ANTHROPIC_API_KEY=
OPENAI_API_KEY=
TAVILY_API_KEY=

# Local Ollama
OLLAMA_BASE_URL=http://localhost:11434

# Gateway (uncomment if using a proxy like LiteLLM)
# GATEWAY_BASE_URL=http://localhost:4000/v1
# GATEWAY_API_KEY=

DEFAULT_MODEL=ollama:llama3.1:8b
DEFAULT_SYSTEM_PROMPT=You are a helpful AI assistant with planning, file management, and research capabilities.
DEFAULT_BACKEND=state
DEFAULT_DEBUG=false

HOST=0.0.0.0
PORT=8000
